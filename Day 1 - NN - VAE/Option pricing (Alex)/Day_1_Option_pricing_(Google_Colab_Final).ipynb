{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "colab": {
      "name": "Day 1 - Option pricing (Google Colab_Final).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kbsjwbn2grN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4MlbRbx2grS",
        "colab_type": "text"
      },
      "source": [
        "### Part 1: Introduction to the deep learning library Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN-oB8Gp2grS",
        "colab_type": "text"
      },
      "source": [
        "#### Goals:\n",
        "- A) Introduction of the Keras library on a high-level for supervised learning tasks.\n",
        "- B) On a toy example (iris dataset), predict the type of flower of the iris dataset (classification) as well as a regression task with a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAzyr7tH2grT",
        "colab_type": "text"
      },
      "source": [
        "A) Importation of the Keras library\n",
        "- Keras is a high-level deep learning library that is considered to be user-friendly, modular and extensible. \n",
        "- Keras can be used to compile different types of deep neural networks which you will see throughout this week: multilayer perceptron (today), convolutional layer (friday), recurrent layer (tomorrow), etc...\n",
        "- The implementation of a neural network with Keras requires a small number of lines of code. The training part (estimation of parameters) is also optimized such that it can be done automatically for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HfnNvRf2grU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, load_model      \n",
        "from keras.layers import Dense       # 'Dense' layer is a fully-connected layer in Keras, i.e. the layer used in an MLP \n",
        "from keras.utils import np_utils     # For the one-hot encoding, see below. \n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVcCV5Yc2grY",
        "colab_type": "text"
      },
      "source": [
        "#### B.1) First supervised learning task\n",
        "- We start with a toy example: the Iris dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAVVLwGY2grY",
        "colab_type": "text"
      },
      "source": [
        "Let's first download the iris dataset\n",
        "- The iris dataset is really simple. It consist of 150 flowers with 4 features: petal length, petal width, sepal length, sepal width and three species of flowers (setosa, virginica and versicolor). \n",
        "- We will try to predict the type of flower based on the 4 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ABcoSK2grZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris_data = load_iris() \n",
        "features = iris_data['data']\n",
        "targets = iris_data['target']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbwozd9V2grc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"examples of features:\")\n",
        "print (features[0:10,:])\n",
        "print(\"species\")\n",
        "print(targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MVBGXUD2gre",
        "colab_type": "text"
      },
      "source": [
        "For a multi-classification task, it's best practice to encode the targets as a 'one-hot' encoding. \n",
        "- A one-hot incoding is simply a vector of zeros everywhere except for a '1' at the position of the class. \n",
        "- Ex: for the iris dataset, we have three classes. A class '0' will be encoded as :[1., 0, 0], a class '1': [0,1,0] and class '2': [0,0,1]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1iAmSxJ2grf",
        "colab_type": "text"
      },
      "source": [
        "One-hot encoding is done in the following box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fir7g63F2grg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_y = np_utils.to_categorical(targets)\n",
        "print(dummy_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V657UHC2grj",
        "colab_type": "text"
      },
      "source": [
        "Let's split our dataset into a Train|Test set of 70%/30%. \n",
        "- This is done with the function 'train_test_split'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1BeCasg2grj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features, dummy_y, test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m13lFgtw2grm",
        "colab_type": "text"
      },
      "source": [
        "Let's compile our first neural network model to predict the type of flower.\n",
        "- Single hidden layer of 5 neurons each with activation function 'tanh'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UPAhu51n2grn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Input layer: 5 hidden neurons and 'tanh' activation function\n",
        "model.add(Dense(units=5, activation='tanh', input_dim=x_train.shape[-1]))\n",
        "\n",
        "# Output layer: need to use softmax as we have a classification problem\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model by specifying the loss function (cross entropy), the optimizer (Adam) and optional metric to output ('accuracy')\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHeZS-Fd2grp",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the performance on the Train|Test set from the resulting neural network\n",
        "- In the context of multi-classification, the function 'predict' of Keras takes as input a set of features, and outputs the probability of being in each class.\n",
        "- i.e. model.predict(x_train) outputs a discrete probability distribution over the 3 types of flowers for each example in our training set. \n",
        "- The predicted flower will be the highest probability class (the mode)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XgqPxW_2grq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_train = model.predict(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyHF9Nao2grs",
        "colab_type": "text"
      },
      "source": [
        "Let's analyze the predicted discrete probability distribution for the first 5 examples in the train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkb7cKvJ2grs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Probability distribution for the first 5 examples of the Train set:\")\n",
        "print(y_pred_train[0:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZMjCdob2grv",
        "colab_type": "text"
      },
      "source": [
        "and the predicted class for the same five examples vs the true label (true type of flower). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW4AMnVi2grv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Predicted flower for the first 5 examples (goes from 0 to 2):\")\n",
        "print(np.argmax(y_pred_train[0:5],axis=1))\n",
        "print(\"True type of flowers for the first 5 examples\")\n",
        "print(np.argmax(y_train[0:5],axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOM1MBlC2grz",
        "colab_type": "text"
      },
      "source": [
        "Let's compute the accuracy on the train and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk3NvL4m2gr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Train set accuracy:\")\n",
        "print(np.sum(np.argmax(y_pred_train, axis=1)==np.argmax(y_train, axis=1))/x_train.shape[0])\n",
        "print(\"Number of errors on the Train set out of %d examples:\" %(x_train.shape[0]))\n",
        "print(x_train.shape[0] - np.sum(np.argmax(y_pred_train, axis=1)==np.argmax(y_train, axis=1)))\n",
        "\n",
        "print(\"Test set accuracy:\")\n",
        "y_pred_test = model.predict(x_test)\n",
        "print(np.sum(np.argmax(y_pred_test, axis=1)==np.argmax(y_test, axis=1))/x_test.shape[0])\n",
        "print(\"Number of errors on the Test set out of %d examples:\" %(x_test.shape[0]))\n",
        "print(x_test.shape[0] - np.sum(np.argmax(y_pred_test, axis=1)==np.argmax(y_test, axis=1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8835Ksf2gr2",
        "colab_type": "text"
      },
      "source": [
        "#### B.2) Predict the sepal width based on the other features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOmcRuTz2gr3",
        "colab_type": "text"
      },
      "source": [
        "Still with the 'iris' dataset, let's predict the value of the 'sepal width' (the 4th feature) based on the other three other. \n",
        "- This is a regression task, as the 4th feature is a real value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9TuLrbG2gr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_without_sepal_width = features[:,:-1]\n",
        "sepal_width = features[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cjg3OsI2gr5",
        "colab_type": "text"
      },
      "source": [
        "Again, let's split our dataset into a Train|Test set of 70%. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anBBG3zE2gr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features_without_sepal_width, sepal_width, test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY69YaBn2gr8",
        "colab_type": "text"
      },
      "source": [
        "Let's compile our neural network. Notice two important differences:\n",
        "- 1) Output layer: must be of size 1 and the activation is no longer 'softmax', it's a 'linear' activation function.\n",
        "- 2) Loss function: mean-square error (MSE) instead of cross-entropy. MSE is the default loss function to use in most regression tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-gsmJhiY2gr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Input layer: 5 hidden neurons and 'tanh' activation function\n",
        "model.add(Dense(units=5, activation='tanh', input_dim=x_train.shape[-1]))\n",
        "\n",
        "# Output layer: default activation function is 'linear'\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model by specifying the loss function (MSE), the optimizer (Adam).\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x_train,y_train,epochs=50,batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeMSl9s-2gr_",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the performance on the Train|Test set from the resulting neural network.\n",
        "- In the context of a regression task, the function 'predict' of Keras takes as input a set of features, and outputs a scalar that represents the prediction of the target (sepal width)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EskjXz5k2gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_train = model.predict(x_train)\n",
        "print(\"Predicted sepal width for the first 5 examples\")\n",
        "print(y_pred_train[0:5])\n",
        "print(\"True sepal width for the first 5 examples\")\n",
        "print(y_train[0:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf8AIhm92gsD",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the total performance on the Train and Test set of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU3uBgDv2gsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Mean-square error obtained on the Train set:\")\n",
        "print(np.average((y_pred_train[:,0]-y_train)**2))\n",
        "\n",
        "print(\"Mean-square error obtained on the Test set:\")\n",
        "y_pred_test = model.predict(x_test)\n",
        "print(np.average((y_pred_test[:,0]-y_test)**2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr6a3bWO2gsG",
        "colab_type": "text"
      },
      "source": [
        "### Part 2) Option pricing with neural networks\n",
        "In this second part, we will try to price options with neural networks under the Black-Scholes model. Many papers can be found doing a similar task, see for example https://srdas.github.io/Papers/BlackScholesNN.pdf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfJ_5k6A2gsH",
        "colab_type": "text"
      },
      "source": [
        "### Sections:\n",
        "- 2.1) Simulation of the dataset\n",
        "- 2.2) Normalization of the features\n",
        "- 2.3) Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYwAO_N2gsM",
        "colab_type": "text"
      },
      "source": [
        "### 2.1) Simulation of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4b8hfFT2gsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to compute the true price of a European call option under the Black-Scholes model\n",
        "# - S    : price of the underlying\n",
        "# - K    : strike price\n",
        "# - T    : time-to-maturity\n",
        "# - q    : continuous dividend yield\n",
        "# - sigma: volatility parameter\n",
        "# - r    : continuous risk-free rate\n",
        "def BlackScholes_price(S, K, T, q, sigma, r):\n",
        "    d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
        "    d2 = d1 - sigma * np.sqrt(T)\n",
        "    return S * np.exp(-q * T) * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hisHwLZN2gsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to generate the dataset of pairs (X,Y) where X = [S,K,T,r,sigma,q] and Y = call price.\n",
        "# - n: number of datapoints\n",
        "def generate_options(n):\n",
        "        data = {}\n",
        "        data['S'] = np.random.uniform(1, 3000, n)               # Uniformly sampled from [1,3000]\n",
        "        data['K'] = data['S'] * np.random.uniform(0.7, 1.3, n)  # Strike price should be around the stock price\n",
        "        data['T'] = np.random.uniform(0, 2, n)                  # Number of years\n",
        "        data['r'] = np.random.uniform(0, 0.2, n)                # Continuous risk-free rate\n",
        "        data['sigma'] = np.random.uniform(0, 1, n)              # volatility \n",
        "        data['q'] = np.random.uniform(0, 0.2, n)                # continuous dividend yield\n",
        "        data['P'] = BlackScholes_price(**data)                  # Option price\n",
        "\n",
        "        return pd.DataFrame.from_dict(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC8doxUy2gsM",
        "colab_type": "text"
      },
      "source": [
        "In this part, we simulate a dataset of pairs (X,Y) where X = [S,K,T,q,r,sigma] are the features needed under the BSM to compute the price (Y) of a call option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB0e44iz2gsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a dataset of call options from the BSM\n",
        "Datasettraining = generate_options(int(1 * 1E5))                        # Generate 100K call options from BSM\n",
        "df = Datasettraining.copy(deep=True)                                    # Copy of the training set\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER-aRMSb2gsR",
        "colab_type": "text"
      },
      "source": [
        "In this next box, the simulated dataset is splitted into a train|valid|test set of proportion 0.6|0.2|0.2\n",
        "- The train set will be used to fit the parameters of the neural network;\n",
        "- The valid set will be used for hyperparameters tuning;\n",
        "- The test set will be used to evaluate the out-of-sample performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pzoFS6f2gsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)  # Shuffle the dataset\n",
        "\n",
        "# 1) Split into Train|Valid|Test set\n",
        "# Proportions: 0.6|0.2|0.2\n",
        "n = len(df)\n",
        "prop_train = 0.8*0.75\n",
        "prop_valid = 0.8*0.25\n",
        "prop_test = 0.2\n",
        "n_train = (int)(prop_train*n)\n",
        "n_valid = int(prop_valid*n)\n",
        "n_test = int(prop_test*n)\n",
        "\n",
        "# Split\n",
        "train = df[0:n_train]\n",
        "valid = df[n_train:n_train+n_valid]\n",
        "test = df[n_train+n_valid:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-uLJRdk2gsU",
        "colab_type": "text"
      },
      "source": [
        "### 2.2) Normalization of the features\n",
        "- The features ['S', 'K', 'r', 'T', 'sigma' 'q'] are each continuous random variables (i.e. they are either positive real values or real values).\n",
        "- One approach to normalize the features is with the Z-normalization, i.e. standardized each feature by substracting its mean and dividing by its standard deviation (std). \n",
        "- Note: it's good practice to estimate the mean and std of each feature with the training set: \n",
        "    - i.e. we normalize each feature of the train|valid|test sets with the mean and std computed stricly on the train set.\n",
        "    - This should help improve the generalization error (i.e. difference in performance between train and test sets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjxX1M7W2gsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize features\n",
        "# 1) Features to normalize with Z-normalization\n",
        "X_train = train[['S', 'K', 'r', 'T', 'sigma', 'q']].values\n",
        "X_valid = valid[['S', 'K', 'r', 'T', 'sigma', 'q']].values\n",
        "X_test = test[['S', 'K', 'r', 'T', 'sigma', 'q']].values\n",
        "\n",
        "# 2) Z-normalization step\n",
        "# 2.1) Compute the mean/std on the TRAIN set only\n",
        "train_mean = np.mean(X_train, axis=0)\n",
        "train_std  = np.std(X_train, axis=0)\n",
        "\n",
        "# 2.2) Z-normalize the train|valid|test with the mean/std of the train set\n",
        "xtrain = (X_train - train_mean)/train_std\n",
        "xvalid = (X_valid - train_mean)/train_std\n",
        "xtest  = (X_test - train_mean)/train_std\n",
        "\n",
        "# 2.3) Store the targets (i.e. option prices)\n",
        "ytrain = train['P'].values                                   \n",
        "yvalid = valid['P'].values \n",
        "ytest  = test['P'].values "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGiw9Sj2gsW",
        "colab_type": "text"
      },
      "source": [
        "### 2.3) Optimization (without hyperparameter tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkeNytVk2gsX",
        "colab_type": "text"
      },
      "source": [
        "In this first part, we train a neural network to price call options without hyperparameters tuning.\n",
        "- Total number of epoch      : 30\n",
        "- Batch size                 : 64\n",
        "- Number of hidden layers    : 2\n",
        "- Number of neurons per layer: 32\n",
        "- Activation function        : Relu\n",
        "- Loss                       : Mean-squared error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "p3H5muxI2gsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_epoch = 30     # total number of epochs\n",
        "batch_size = 64   # mini-batch size\n",
        "nbs_neurons = 32  # number of neurons per hidden layer\n",
        "\n",
        "# Compile the model  - 2 hidden layers of Relu\n",
        "model = Sequential()\n",
        "model.add(Dense(units = nbs_neurons, activation = 'relu', input_dim = xtrain.shape[1]))\n",
        "model.add(Dense(units = nbs_neurons, activation = 'relu'))\n",
        "\n",
        "# Output layer \n",
        "model.add(Dense(units=1, activation = 'relu'))\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Print the structure of the neural network\n",
        "model.summary()\n",
        "\n",
        "# Train the neural network\n",
        "model.fit(xtrain, ytrain, epochs=nb_epoch, batch_size=batch_size, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01Fc6Qit9psA",
        "colab_type": "text"
      },
      "source": [
        "It's always good practice to save the resulting trained neural network. Once saved, we can always reload the trained neural network for future use without having to retrain it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJG5zpkD9xLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the resulting trained neural network\n",
        "model.save(\"model_no_tuning.h5\")\n",
        "\n",
        "# Example of how to load the trained model\n",
        "model_no_tuning = load_model(\"model_no_tuning.h5\")\n",
        "model_no_tuning.summary()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_VH_MUG8v5w",
        "colab_type": "text"
      },
      "source": [
        "Now that the model is trained, we can print out the resulting performance in terms of the MSE on the train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0SD9lD49nAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1) Compute the predicted call price on the train and test set\n",
        "y_pred_train = model_no_tuning.predict(xtrain).flatten()  # train set predictions\n",
        "y_pred_test  = model_no_tuning.predict(xtest).flatten()   # test set predictions\n",
        "\n",
        "# 2) Some examples of the predictions on the test set\n",
        "print(\"First 5 predictions:\")\n",
        "print(y_pred_test[0:5])\n",
        "print(\"First 5 real values:\")\n",
        "print(ytest[0:5])\n",
        "\n",
        "# 3) Compute the MSE on the train and test sets\n",
        "MSE_train = np.mean(np.square(y_pred_train - ytrain))\n",
        "print(\"With no hyperparameters tuning, the MSE on the train set is: %.5f\" %(MSE_train))\n",
        "\n",
        "MSE_test = np.mean(np.square(y_pred_test - ytest))\n",
        "print(\"With no hyperparameters tuning, the MSE is: %.5f\" %(MSE_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT3EIDyr-0yf",
        "colab_type": "text"
      },
      "source": [
        "Since the MSE on the train and test set are relatively close, there's no indication that the neural network overfitted the training set. This is not suprising since we didn't train much (i.e. only 30 epochs).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J8GU0om-NHn",
        "colab_type": "text"
      },
      "source": [
        "For the current problem of option pricing, it's also interesting to do a scatter plot of the predicted values vs the real values. The scatter plot is a visual way to quickly assess the prediction performance of the neural network. The best case scenario would be for the points to all fall on the diagonal line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riiE8Skh-UQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1) Scatter plot of the train set\n",
        "plt.scatter(ytrain, y_pred_train, s=1.5)\n",
        "plt.title('No hyperparameters tuning - train set')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "# 2) Scatter plot of the test set\n",
        "plt.scatter(ytest, y_pred_test, s=1.5)\n",
        "plt.title('No hyperparameters tuning - test set')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "966hV9v__rpe",
        "colab_type": "text"
      },
      "source": [
        "As we can see, we are making a lot of errors, especially for lower call prices. \n",
        "---\n",
        "One factor that could explain this result is that the mean-square error penalizes larger errors, which initially (i.e. at the beginning of the training) will happen most often for high values of call options. Thus, in order to minimize the MSE, the neural network has to focus more on the pricing of large values of call options. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcnpLsXh2gsc",
        "colab_type": "text"
      },
      "source": [
        "### Part 3) Optimization (with hyperparameter tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCk93wlK2gsc",
        "colab_type": "text"
      },
      "source": [
        "### A) Grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aI2vbR22gsd",
        "colab_type": "text"
      },
      "source": [
        "As the first example of hyperparameters tuning, we will use a well-known method called 'grid search'. \n",
        "- This method consist in testing each combination of hyperparameter on a grid of possible values for each hyperparameter. In this  case, we will test the following grid:\n",
        "    - {2,3} layers\n",
        "    - {100,120} neurons/layer\n",
        "    - {128,256} batch_size\n",
        "- So, we test a total of 8 combinations of hyperparameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2In_KxS2gsd",
        "colab_type": "text"
      },
      "source": [
        "The resulting trained model will be the one that minimizes the error on the validation set out of all of the combinations tested. The latter model will be saved as 'best_model_grid_search.h5'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tASz6SzW2gse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_epoch = 20   # total number of epochs\n",
        "\n",
        "# Grid search\n",
        "nbs_layers_range  = np.array([2,3])       # 2 or 3 hidden layers\n",
        "nbs_neurons_range = np.array([100,120])   # for each hidden layer, either {100,120,140} hidden neurons\n",
        "batch_size_range  = np.array([128,256])   # batch size: either {128,256}\n",
        "\n",
        "# During the optimization, we keep track of the best validation set error obtained\n",
        "valid_loss_best  = 999999   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sN08_z52gsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loop over the different combination on the grid\n",
        "for i in range(len(nbs_layers_range)):\n",
        "    nbs_layer = nbs_layers_range[i]\n",
        "    for j in range(len(nbs_neurons_range)):\n",
        "        nbs_neurons = nbs_neurons_range[j]\n",
        "        for k in range(len(batch_size_range)):\n",
        "            batch_size = batch_size_range[k]\n",
        "            \n",
        "            print(\"Current set of HP: %d hidden layers, %d number of neurons, %d batch size\" %(nbs_layer, nbs_neurons, batch_size))\n",
        "            \n",
        "            # Compile the model:\n",
        "            model = Sequential()\n",
        "            model.add(Dense(units = nbs_neurons, activation = 'relu', input_dim = xtrain.shape[1]))\n",
        "            model.add(Dense(units = nbs_neurons, activation = 'relu'))\n",
        "            \n",
        "            # Check if we have to add a third hidden layer on top of the first two.\n",
        "            if(nbs_layer == 3):\n",
        "                model.add(Dense(units = nbs_neurons, activation = 'relu'))\n",
        "            \n",
        "            # Output layer\n",
        "            model.add(Dense(units=1, activation = 'relu'))\n",
        "            model.compile(loss='mse', optimizer='adam')\n",
        "            \n",
        "            for h in range(nb_epoch):\n",
        "                model.fit(xtrain, ytrain, epochs=1, batch_size=batch_size, verbose = 1)\n",
        "            \n",
        "                # Evaluate the validation loss of the trained model\n",
        "                val_loss = model.evaluate(xvalid, yvalid)\n",
        "               \n",
        "                # If it's the best model so far:\n",
        "                if (val_loss < valid_loss_best):\n",
        "                    valid_loss_best = val_loss\n",
        "                    model.save(\"best_model_grid_search.h5\")\n",
        "\n",
        "# load and print out the best model found with grid search\n",
        "model_best_grid_search = load_model(\"best_model_grid_search.h5\")\n",
        "model_best_grid_search.summary()  \n",
        "\n",
        "# Evaluate performance on train set\n",
        "y_pred_train = model_best_grid_search.predict(xtrain).flatten()\n",
        "MSE_train = np.mean(np.square(y_pred_train - ytrain))\n",
        "print(\"With grid search, the MSE on the train set is: %.5f\" %(MSE_train))\n",
        "\n",
        "# Plot predicted prices vs real prices\n",
        "plt.scatter(ytrain, y_pred_train, s=1.5)\n",
        "plt.title('Grid search - Train set')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate performance on test set\n",
        "y_pred_test = model_best_grid_search.predict(xtest).flatten()\n",
        "MSE_test = np.mean(np.square(y_pred_test - ytest))\n",
        "print(\"With grid search, the MSE on the test set is: %.5f\" %(MSE_test))\n",
        "\n",
        "# Plot predicted prices vs real prices\n",
        "plt.scatter(ytest, y_pred_test, s=1.5)\n",
        "plt.title('Grid search - Test set')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZouHr6K2gsh",
        "colab_type": "text"
      },
      "source": [
        "### B) Random search algorithm for model tuning\n",
        "- Define a space of possible HPs from which we want to find an optimal set.\n",
        "- Random search: \n",
        "    - For a fix number of trials, sample randomly a set of HPs from the defined space;\n",
        "    - Choose the set of HPs which minimizes the MSE on the valid set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0IBC1Nr2gsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128   # Fix the batch size in this case\n",
        "\n",
        "# Search space - define the boundaries for each HP\n",
        "nbs_layers_range = np.array([2,4])             # number of layers within {2,3,4} \n",
        "nbs_neurons_range = np.array([100,400])        # number of neurons within {100,101,102,....,400}\n",
        "lr_range = np.array([0.0001,0.01])             # learning rate within [0.0001, 0.01]\n",
        "\n",
        "# Statistics to keep track of during the optimization\n",
        "valid_loss_best = 999999\n",
        "\n",
        "# Total number of iterations to be done in the random search\n",
        "nbs_iteration = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQxyy7VG2gsk",
        "colab_type": "text"
      },
      "source": [
        "Random search implementation with 5 iterations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO4DV5wR2gsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(nbs_iteration):\n",
        "    \n",
        "    # 1) At the beginning of each iteration, randomly sample a set of HP's\n",
        "    nbs_layers = np.random.randint(low=nbs_layers_range[0], high=nbs_layers_range[1]+1)\n",
        "    nbs_neurons = np.random.randint(low=nbs_neurons_range[0], high=nbs_neurons_range[1]+1)\n",
        "    learning_rate = np.random.uniform(low=lr_range[0], high=lr_range[1])\n",
        "    print(\"Iteration %d --- Nbs layers: %d, Nbs neurons: %d, learning rate: %.4f\" % (i+1, nbs_layers, nbs_neurons, learning_rate))\n",
        "    \n",
        "    # 2) Compile the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units = nbs_neurons, activation = 'relu', input_dim = xtrain.shape[1]))\n",
        "    model.add(Dense(units = nbs_neurons, activation = 'relu'))\n",
        "            \n",
        "    # Check if we have to add a third hidden layer on top of the first two.\n",
        "    if(nbs_layers == 3):\n",
        "        model.add(Dense(units = nbs_neurons, activation = 'relu'))\n",
        "    \n",
        "    # Check if we have to add a third and forth hidden layers on top of the first two.\n",
        "    if(nbs_layers ==4):\n",
        "        model.add(Dense(units = nbs_neurons, activation = 'relu'))\n",
        "        model.add(Dense(units = nbs_neurons, activation = 'relu'))\n",
        "            \n",
        "    # Output layer\n",
        "    model.add(Dense(units=1, activation = 'relu'))\n",
        "    \n",
        "    sgd = optimizers.Adam(lr=learning_rate)   # watch out - we also sample the learning rate!\n",
        "    model.compile(loss='mse', optimizer=sgd)\n",
        "    \n",
        "    for h in range(nb_epoch):\n",
        "        model.fit(xtrain, ytrain, epochs=1, batch_size=batch_size, verbose = 1)\n",
        "            \n",
        "        # Evaluate the validation loss of the trained model\n",
        "        val_loss = model.evaluate(xvalid, yvalid)\n",
        "               \n",
        "        # If it's the best model so far:\n",
        "        if (val_loss < valid_loss_best):\n",
        "            valid_loss_best = val_loss\n",
        "            model.save(\"best_model_random_search.h5\")\n",
        "    \n",
        "# load and print out the best model found with grid search\n",
        "model_best_random_search = load_model(\"best_model_random_search.h5\")\n",
        "model_best_random_search.summary()  \n",
        "\n",
        "# Evaluate performance on train set\n",
        "y_pred_train = model_best_random_search.predict(xtrain).flatten()\n",
        "MSE_train = np.mean(np.square(y_pred_train - ytrain))\n",
        "print(\"With random search, the MSE on the train set is: %.5f\" %(MSE_train))\n",
        "\n",
        "# Plot predicted prices vs real prices\n",
        "plt.scatter(ytrain, y_pred_train, s=1.5)\n",
        "plt.title('Random search - Train set')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate performance on test set\n",
        "y_pred_test = model_best_random_search.predict(xtest).flatten()\n",
        "MSE_test = np.mean(np.square(y_pred_test - ytest))\n",
        "print(\"With random search, the MSE is: %.5f\" %(MSE_test))\n",
        "\n",
        "# Plot predicted prices vs real prices\n",
        "plt.scatter(ytest, y_pred_test, s=1.5)\n",
        "plt.title('Random search')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxsTV9KN2gsn",
        "colab_type": "text"
      },
      "source": [
        "Let's summarize the results of the three approaches (no HP tuning, grid search and random search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS1bX0Jf2gsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1) No HP tuning\n",
        "y_pred_no_tune = model_no_tuning.predict(xtest).flatten()\n",
        "MSE_test = np.mean(np.square(y_pred_no_tune - ytest))\n",
        "print(\"With no tuning, the MSE on the test set is: %.5f\" %(MSE_test))\n",
        "\n",
        "# 2) Grid search\n",
        "y_pred_grid = model_best_grid_search.predict(xtest).flatten()\n",
        "MSE_test = np.mean(np.square(y_pred_grid - ytest))\n",
        "print(\"With grid search, the MSE on the test set is: %.5f\" %(MSE_test))\n",
        "\n",
        "# 3) Random search \n",
        "y_pred_random = model_best_random_search.predict(xtest).flatten()\n",
        "MSE_test = np.mean(np.square(y_pred_random - ytest))\n",
        "print(\"With random search, the MSE on the test set is: %.5f\" %(MSE_test))\n",
        "\n",
        "# 4) Plot scatter of each\n",
        "plt.scatter(ytest, y_pred_no_tune, s=1.5)\n",
        "plt.title('No hyperparameters tuning')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(ytest, y_pred_no_tune, s=1.5)\n",
        "plt.title('Grid search')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(ytest, y_pred_random, s=1.5)\n",
        "plt.title('Random search')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKsMbx6xeko5",
        "colab_type": "text"
      },
      "source": [
        "Based on our results, we can make the following qualitative observations:\n",
        "-\t1) Hyperparameter tuning can improve significantly the performance of our model. Indeed, both grid search and random search improved upon the MSE obtained with no hyperparameter tuning.\n",
        "-\t2) Prior to the optimization, it's not clear which of grid search of random search will provide the best solution.\n",
        "-\t3) The key takeaway: over the long-run, for the same computational budget, random search tends to provide a better solution than grid search, i.e. there's a higher probability that random search will find a better set of hyperparameters than grid search for a fixed computational budget. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GG0VHG12gsp",
        "colab_type": "text"
      },
      "source": [
        "### 4) Other popular approaches and 'homework'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmombdMI2gsq",
        "colab_type": "text"
      },
      "source": [
        "In this last part, we will implement two popular regularization methods to potentially improve our results: dropout and batchnormalization. In the next few boxes, we give simple examples of how to use dropout and batchnormalization with Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XeGiYd_2gsr",
        "colab_type": "text"
      },
      "source": [
        "Afterwards, given the time left, you will implement yourself an optimization approach of your choice (grid search, random search etc.) with the use of dropout and/or batchnormalization to see if you can improve upon the results obtained so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfypI9_S2gss",
        "colab_type": "text"
      },
      "source": [
        "### A) Dropout\n",
        "- Dropout is typically added after the activation function (i.e. after the hidden layer).\n",
        "- The input of the dropout function is the fraction of units to drop (value between (0,1)).\n",
        "    - What fraction of the units should we drop? Can be considered as an additional hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GkO8KioB2gss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dropout  # Import the Dropout layer from Keras\n",
        "\n",
        "nb_epoch = 30      # total number of epochs\n",
        "batch_size = 128   # mini-batch size\n",
        "nbs_neurons = 150  # number of neurons per layer\n",
        "\n",
        "# Compile the model  - 2 hidden layers of Relu\n",
        "model_dropout_ex = Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "model_dropout_ex.add(Dense(units = nbs_neurons, activation = 'relu', input_dim = xtrain.shape[1]))\n",
        "model_dropout_ex.add(Dropout(0.5))   # we drop 50% of the units\n",
        "\n",
        "# Second hidden layer\n",
        "model_dropout_ex.add(Dense(units = nbs_neurons, activation = 'relu'))\n",
        "model_dropout_ex.add(Dropout(0.5))   # we drop 50% of the units\n",
        "\n",
        "# Output layer \n",
        "model_dropout_ex.add(Dense(units=1, activation = 'relu'))\n",
        "model_dropout_ex.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Print the structure of the neural network\n",
        "model_dropout_ex.summary()\n",
        "\n",
        "# Train the model\n",
        "model_dropout_ex.fit(xtrain, ytrain, epochs=nb_epoch, batch_size=batch_size, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLhk0jqE2gsu",
        "colab_type": "text"
      },
      "source": [
        "What do you observe? Did dropout help? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26SvRtIN2gsx",
        "colab_type": "text"
      },
      "source": [
        "### B) Batchnormalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SkuXUZQJ2gsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "\n",
        "model_BN_ex = Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "model_BN_ex.add(Dense(units = nbs_neurons, activation = 'relu', input_dim = xtrain.shape[1]))\n",
        "model_BN_ex.add(BatchNormalization())  # Batchnormalization is applied on the first hidden layer\n",
        "\n",
        "# Second hidden layer\n",
        "model_BN_ex.add(Dense(units = nbs_neurons, activation = 'relu'))\n",
        "model_BN_ex.add(BatchNormalization())  # Batchnormalization is applied on the second hidden layer\n",
        "\n",
        "# Output layer \n",
        "model_BN_ex.add(Dense(units=1, activation = 'relu'))\n",
        "model_BN_ex.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Print the structure of the neural network\n",
        "model_BN_ex.summary()\n",
        "\n",
        "# Train the model\n",
        "model_BN_ex.fit(xtrain, ytrain, epochs=nb_epoch, batch_size=batch_size, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWJttS4r2gs0",
        "colab_type": "text"
      },
      "source": [
        "Again, what do you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg4TRHC12gs0",
        "colab_type": "text"
      },
      "source": [
        "It seems that neither batchnormalization nor dropout is useful in this case. You will see examples where dropout and batchnormalization helps training your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wls8MoVf2gs1",
        "colab_type": "text"
      },
      "source": [
        "### C) Your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd22mpBr2gs1",
        "colab_type": "text"
      },
      "source": [
        "Here, try anything you've learned so far to try and beat the best results you've obtained so far! Some suggestions:\n",
        "- 1) Train for more epochs (this will surely help you get better results);\n",
        "- 2) Can dropout and/or batchnorm help when used with random search/grid search?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpVkJYdP2gs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your implementation here"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}