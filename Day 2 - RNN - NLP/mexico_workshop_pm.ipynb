{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mexico_workshop_pm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-yMrrL056XW",
        "colab_type": "code",
        "outputId": "034cd07c-572e-471c-e8fa-f5b5674de432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install tensorflow==1.1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.1.0 in /usr/local/lib/python2.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.1.0) (1.16.4)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.1.0) (2.0.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.1.0) (3.7.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.1.0) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.1.0) (1.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.1.0) (0.15.5)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.1.0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.1.0) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.2.0->tensorflow==1.1.0) (42.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXUBW7BVNDC4",
        "colab_type": "code",
        "outputId": "d0bc9d37-1d46-4239-eb17-82823e7dbb3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6szXy__QYym",
        "colab_type": "text"
      },
      "source": [
        "### What does our data look like?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbiGaV6dk_me",
        "colab_type": "code",
        "outputId": "c3cebf2f-6048-4b57-b440-5a4ddd50c969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!cd /content/drive/My\\ Drive/WORKSHOP\n",
        "# get the dataset from this link: https://docs.google.com/file/d/0B04GJPshIjmPRnZManQwWEdTZjg/edit\n",
        "# put the files in CharLSTM/datasets\n",
        "\n",
        "PATH = '/content/drive/My Drive/WORKSHOP'\n",
        "train_set = '{}/CharLSTM/datasets/training.1600000.processed.noemoticon.csv'.format(PATH)\n",
        "test_set = '{}/CharLSTM/datasets/testdata.manual.2009.06.14.csv'.format(PATH)\n",
        "\n",
        "with open(train_set, 'r') as f:\n",
        "    file = f.readlines()\n",
        "    print('length training set: {}'.format(len(file)))\n",
        "    print(file[0])\n",
        "\n",
        "with open(test_set, 'r') as f:\n",
        "    file = f.readlines()\n",
        "    print('length test set: {}'.format(len(file)))\n",
        "    print(file[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length training set: 1600000\n",
            "\"0\",\"1467810369\",\"Mon Apr 06 22:19:45 PDT 2009\",\"NO_QUERY\",\"_TheSpecialOne_\",\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"\n",
            "\n",
            "length test set: 498\n",
            "\"4\",\"3\",\"Mon May 11 03:17:40 UTC 2009\",\"kindle2\",\"tpryan\",\"@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCjO-EsUQgav",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Datasets / Preprocessing (Optional) \n",
        "\n",
        "Datasets are already created for you, thus, you don't need to run it, but you can keep this function for future reference. Here's what it does:\n",
        "\n",
        "i. Shuffle the dataset\n",
        "\n",
        "ii. Reshape the lines as tuples: (sentiment (0 or 1), sentence) \n",
        "\n",
        "iii. Remove non readable sentences\n",
        "\n",
        "TODO: Remove links, hashtags and references to other twitter users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqPqYifQQ4Bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd /content/drive/My\\ Drive/WORKSHOP\n",
        "import random\n",
        "import csv\n",
        "\n",
        "VALID_PERC = 0.05\n",
        "\n",
        "\n",
        "def reshape_lines(lines):\n",
        "    data = []\n",
        "    for l in lines:\n",
        "        split = l.split('\",\"')\n",
        "        content = (split[0][1:], split[-1][:-2])\n",
        "        try:\n",
        "          word_tokenize(content[1])\n",
        "          data.append(content)\n",
        "        except:\n",
        "          pass\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_csv(out_file, data):\n",
        "    with open(out_file, 'wb') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(data)\n",
        "    print('Data saved to file: %s' % out_file)\n",
        "\n",
        "\n",
        "def shuffle_datasets(valid_perc=VALID_PERC):\n",
        "    \"\"\" Shuffle the dataset \"\"\"\n",
        "\n",
        "    # Create training and validation set\n",
        "    print('Creating training & validation set...')\n",
        "\n",
        "    with open(train_set, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        random.shuffle(lines)\n",
        "        lines_train = lines[:int(len(lines) * (1 - valid_perc))]\n",
        "        lines_valid = lines[int(len(lines) * (1 - valid_perc)):]\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    save_csv('valid_set.csv', reshape_lines(lines_valid))\n",
        "    save_csv('train_set.csv', reshape_lines(lines_train))\n",
        "\n",
        "    print('Creating testing set...')\n",
        "\n",
        "    with open(test_set, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        random.shuffle(lines)\n",
        "\n",
        "        save_csv('test_set.csv', reshape_lines(lines))\n",
        "    print('All datasets have been created!')\n",
        "\n",
        "# shuffle_datasets()\n",
        "# Once the files are created, download them and put them in /content/drive/My\\ Drive/WORKSHOP\n",
        "# We're forced to do it manually because colab does not let you write that directory... I think?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBEtFgLcU8VH",
        "colab_type": "text"
      },
      "source": [
        "# Building the Embedding\n",
        "\n",
        "3 Main Advantages of Working with Character-Level Embeddings:\n",
        "\n",
        "i. The model will be much smaller (around ~50-100 mb for the whole model compared to over 3GB for the classic Word2Vec from Google - this is only taking into account the embedding, not the actual model);\n",
        "\n",
        "ii. The model can understand the underlying emotion of repetitive letters (e.g. hellooooo!!);\n",
        "\n",
        "iii. The model is almost immune to typos.\n",
        "\n",
        "## I. Defining an Alphabet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM5DkFiWU9u5",
        "colab_type": "code",
        "outputId": "2e6b376b-f873-433f-f64d-4e983b613a92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# We're going to embed characters by their position in the ascii \n",
        "# table\n",
        "\n",
        "characters = 'abcdefghijklmnopqrstuvwxyz0123456789-,;' \\\n",
        "                 '.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{} '\n",
        "characters_ascii = np.frombuffer(np.array(list(characters)),\n",
        "                                           dtype=np.uint8) - 32\n",
        "\n",
        "print('The 70 most common characters can be represented as:')\n",
        "print(['{}: {}'.format(characters[i], characters_ascii[i]) for i in range(len(characters))])\n",
        "\n",
        "# You can then one-hot encode a word using the following code (In this example, we encode\n",
        "# the 70 most common characters):\n",
        "alphabet_length = np.max(characters_ascii)\n",
        "embedding = (characters_ascii[:, None] == np.arange(alphabet_length)).astype(int)\n",
        "\n",
        "print('\\n')\n",
        "print('Creating one-hot vector from the 70 most common characters:')\n",
        "print(embedding)\n",
        "print('one-hot vector shape: {}'.format(embedding.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 70 most common characters can be represented as:\n",
            "['a: 65', 'b: 66', 'c: 67', 'd: 68', 'e: 69', 'f: 70', 'g: 71', 'h: 72', 'i: 73', 'j: 74', 'k: 75', 'l: 76', 'm: 77', 'n: 78', 'o: 79', 'p: 80', 'q: 81', 'r: 82', 's: 83', 't: 84', 'u: 85', 'v: 86', 'w: 87', 'x: 88', 'y: 89', 'z: 90', '0: 16', '1: 17', '2: 18', '3: 19', '4: 20', '5: 21', '6: 22', '7: 23', '8: 24', '9: 25', '-: 13', ',: 12', ';: 27', '.: 14', '!: 1', '?: 31', ':: 26', \"': 7\", '\": 2', '/: 15', '\\\\: 60', '|: 92', '_: 63', '@: 32', '#: 3', '$: 4', '%: 5', '^: 62', '&: 6', '*: 10', '~: 94', '`: 64', '+: 11', '-: 13', '=: 29', '<: 28', '>: 30', '(: 8', '): 9', '[: 59', ']: 61', '{: 91', '}: 93', ' : 0']\n",
            "\n",
            "\n",
            "Creating one-hot vector from the 70 most common characters:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [1 0 0 ... 0 0 0]]\n",
            "one-hot vector shape: (70, 94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEuIw7InY5RX",
        "colab_type": "text"
      },
      "source": [
        "## II. Encoding a Sentence\n",
        "\n",
        "We need to convert sentences to a tensor of shape `(sentence_length, max_word_length, alphabet_length)`. \n",
        "\n",
        "PS: We want `max_word_length` to be static for every sentences in our batch, otherwise we won't be able to feed it to our tensorflow model, because, in our case, we will need a fixed number of weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh5iYaYSWaid",
        "colab_type": "code",
        "outputId": "d5bc1ce1-06a5-4111-ca46-cb5f380fa38a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!cd /content/drive/My\\ Drive/WORKSHOP\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# word_tokenize('Hello, how are you?') -> ['hello', 'how', 'are', 'you', '?']\n",
        "\n",
        "TRAIN_SET = '/content/drive/My Drive/WORKSHOP/CharLSTM/datasets/train_set.csv'\n",
        "\n",
        "# We set the maximal number of characters per word to 16\n",
        "max_word_length = 16\n",
        "\n",
        "with open(TRAIN_SET, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    \n",
        "    # print first line\n",
        "    print(lines[1])\n",
        "    \n",
        "    # Each lines look like: (label, sentence)\n",
        "    sentence = lines[1].split(',')[1]\n",
        "    \n",
        "    # word_tokenize converts a sentence to a list:\n",
        "    # 'hello, there' -> ['hello', 'there']\n",
        "    sentence = word_tokenize(sentence)\n",
        "    \n",
        "    # We store the embedding in sentence_emb\n",
        "    sentence_emb = np.zeros(shape=(len(sentence), max_word_length, alphabet_length))\n",
        "    print(sentence)\n",
        "\n",
        "\n",
        "    for i, word in enumerate(sentence):\n",
        "\n",
        "        # Transform word to list: 'hi' -> ['h', 'i']\n",
        "        word_list = np.array(list(word))\n",
        "\n",
        "        # Get the index of each character in our dictionary: ['h', 'i'] -> [72, 73]\n",
        "        word_ints = np.frombuffer(word_list, dtype=np.uint8) - 32\n",
        "\n",
        "        # Transform word to one-hot vector of shape (word_length, alphabet_size)\n",
        "        word_emb = (word_ints[:, None] == np.arange(alphabet_length)).astype(int)[:max_word_length]\n",
        "\n",
        "        # Add the embedded word to the embedded sentence tensor of shape (sentence_length, max_word_length, alphabet_size)\n",
        "        # sentence_emb[0] = [0, 0, 0, 0, 0, ..., 0] (shape=(16, 94)) word_emb = [0, 0, 0, 1, ..., 0] (shape=(word_len, 94))\n",
        "        # where word_length <= 16\n",
        "        sentence_emb[i, 0:len(word_list), :] = word_emb\n",
        "        \n",
        "    print(sentence_emb[0, 0])\n",
        "    print(sentence_emb.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "0,@Qdakid718 I want to come home....  \n",
            "\n",
            "['@', 'Qdakid718', 'I', 'want', 'to', 'come', 'home', '...', '.']\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(9, 16, 94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je8tClmqbklx",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Data Reader\n",
        "\n",
        "## I. One-Hot Encoding for Sentences \n",
        "\n",
        "This code is the same thing as above but in a function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJddzA-8bpN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alphabet_length = 94\n",
        "\n",
        "def encode_one_hot(sentence, max_word_length=16):\n",
        "    sentence = word_tokenize(sentence)\n",
        "    sentence_emb = np.zeros(shape=(len(sentence), max_word_length, alphabet_length))\n",
        "\n",
        "    for i, word in enumerate(sentence):\n",
        "\n",
        "        # Transform word to list: 'hi' -> ['h', 'i']\n",
        "        word_list = np.array(list(word))\n",
        "\n",
        "        # Get the index of each character in our dictionary: ['h', 'i'] -> [72, 73]\n",
        "        word_ints = np.frombuffer(word_list, dtype=np.uint8) - 32\n",
        "\n",
        "        # Transform word to one-hot vector of shape (word_length, alphabet_size)\n",
        "        word_emb = (word_ints[:, None] == np.arange(alphabet_length)).astype(int)[:max_word_length]\n",
        "\n",
        "        # Add the embedded word to the embedded sentence tensor of shape (sentence_length, max_word_length, alphabet_size)\n",
        "        sentence_emb[i, 0:len(word_list), :] = word_emb\n",
        "    \n",
        "    return sentence_emb, len(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "347M_4yPcCRD",
        "colab_type": "text"
      },
      "source": [
        "## II. Creating Minibatches\n",
        "\n",
        "`make_minibatch` take as inputs the raw sentences from our dataset. It calls `encode_one_hot` to encode them into one-hot vectors and then insert them in a tensor of shape `(minibatch_size, max_sentence_length, max_word_length, alphabet_length)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDX_I5bYcFNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_minibatch(sentences, minibatch_size=64, max_word_length=16):\n",
        "    \n",
        "    assert len(sentences) == minibatch_size\n",
        "    \n",
        "    # binary_cross_entropy takes 2 inputs y_true (labels) of shape (minibatch_size, #_classes (2))\n",
        "    # and y_hat (predictions) of shape (minibatch_size, #_classes (2))\n",
        "    minibatch_y = np.zeros(shape=(minibatch_size, 2))\n",
        "    x = []\n",
        "    sentence_lengths = []\n",
        "    \n",
        "    for i, sentence in enumerate(sentences):\n",
        "        line = sentence.split(',')\n",
        "        label, s = line[0], line[1]\n",
        "        \n",
        "        # '0' -> negative '4' -> positive we convert these numbers to one-hot vectors\n",
        "        # for tensorflow's binary_cross_entropy method\n",
        "        minibatch_y[i, :] = np.array([0, 1]) if sentence[:1] == '0' else np.array([1, 0])\n",
        "        res = encode_one_hot(s)\n",
        "        x.append(res[0])\n",
        "        sentence_lengths.append(res[1])\n",
        "    \n",
        "    \n",
        "    # Create the minibatch for the sentences\n",
        "    max_sentence_length = np.max(sentence_lengths)\n",
        "    minibatch_x = np.zeros(shape=(minibatch_size, max_sentence_length, max_word_length, alphabet_length))\n",
        "    for i, emb_s in enumerate(x):\n",
        "        minibatch_x[i, 0:len(emb_s)] = emb_s\n",
        "\n",
        "    return minibatch_x, minibatch_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoTRmbhccHjl",
        "colab_type": "code",
        "outputId": "2246d38a-4844-4502-982a-ca99f06c0dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing\n",
        "sentences = lines[:64]\n",
        "m_x, m_y = make_minibatch(sentences)\n",
        "print(m_x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 32, 16, 94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSxfUzVjcGEs",
        "colab_type": "text"
      },
      "source": [
        "III. Building the Iterator\n",
        "The iterator function let's you loop through your dataset and creates the minibatch that you will feed to your model during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy7gf9z5cLYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_minibatch(dataset=TRAIN_SET, minibatch_size=64, max_word_length=16):\n",
        "    with open(dataset, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        n_batch = int(len(lines) // minibatch_size)\n",
        "        \n",
        "        for i in range(n_batch):\n",
        "            \n",
        "            sentences = lines[i * minibatch_size: i * minibatch_size + minibatch_size]\n",
        "            m_x, m_y = make_minibatch(sentences, minibatch_size=minibatch_size,\n",
        "                                     max_word_length=max_word_length)\n",
        "            yield m_x, m_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-oYjgjGcOM5",
        "colab_type": "code",
        "outputId": "54eaa417-0c65-4b3f-b026-4c86ed1ece55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Testing\n",
        "test = next(iterate_minibatch())\n",
        "print(test[0].shape)\n",
        "print(test[1].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 32, 16, 94)\n",
            "(64, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYGhcYWScgdf",
        "colab_type": "text"
      },
      "source": [
        "# Building the Model\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=14QbvpXnYAhZ_DqWXAHbF3Y7YlJltc1hy)\n",
        "\n",
        "\n",
        "\n",
        "## I. Graph Inputs\n",
        "\n",
        "Tensorflow take as inputs numpy arrays, every time you want to evaluate a tensorflow variable you first need to call a session to compile your graph and then feed them your numpy array.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmKnVmSjdivj",
        "colab_type": "code",
        "outputId": "98035f0c-e173-4eb8-b9d4-9dee9c5f24be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "max_word_length = 16\n",
        "alphabet_length = 94\n",
        "minibatch_size = 64\n",
        "\n",
        "# Placeholder for the embedded sentences shape: (minibatch_size, max_sentence_length, max_word_length, alphabet_length)\n",
        "# Remember that max_sentence_length changes with each minibatch\n",
        "x = tf.placeholder('float32', shape=[None, None, max_word_length, alphabet_length], name='X')\n",
        "\n",
        "# Placeholder for the embedded labels (0, 1) -> negative (1, 0) -> positive\n",
        "y = tf.placeholder('float32', shape=[None, 2], name='Y')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8_nIIjrdmE8",
        "colab_type": "code",
        "outputId": "e6ff2eff-f8ce-47cc-c9f0-f1cc0aa3c0a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Testing\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    x_, y_ = sess.run([x, y], feed_dict={x: test[0], y: test[1]})\n",
        "    print(x_.shape)\n",
        "    print(y_.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 32, 16, 94)\n",
            "(64, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA67V8_0dtBx",
        "colab_type": "text"
      },
      "source": [
        "## II. Time-Delay Neural Network\n",
        "\n",
        "The CharCNN model is composed of multiple convolutional layers with different kernel widths (in this implementation, I use 25 x kernels of size 1, 50 x kernels of size 2, …, 175 x kernels of size 7 - height is always the same as the length of our character-alphabet). Each of them take as input ONE word of the sentence at a time (the CharCNN is used as an embedding of the words that will then be fed to the LSTM where each time step in the LSTM is a word of the sentence).\n",
        "\n",
        "After the convolution, we do a max pooling operation for every kernels over the resulting width of the convolution, this operation acts as a sort of arrangement of the most important features of the word for every n-grams. For example, a kernel with a width of 4 might have learned to detect repetitive characters and would “fire” when it sees “oooo” in “helloooo!”.\n",
        "\n",
        "You can learn more about this model [here](https://arxiv.org/pdf/1508.06615.pdf).\n",
        "\n",
        "![](https://drive.google.com/uc?id=1IpdS5nAw6_2_Z3UXMnPbhs_EwEzzi0yI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-k3Qmp-dzfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(input_, output_dim, k_h, k_w, name=\"conv2d\"):\n",
        "    with tf.variable_scope(name):\n",
        "\n",
        "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim])\n",
        "        b = tf.get_variable('b', [output_dim])\n",
        "\n",
        "    return tf.nn.conv2d(input_, w, strides=[1, 1, 1, 1], padding='VALID') + b\n",
        "\n",
        "kernels=[1, 2, 3, 4, 5, 6, 7]\n",
        "kernel_features=[25, 50, 75, 100, 125, 150, 175]\n",
        "\n",
        "def tdnn(input_, kernels, kernel_features, scope='TDNN'):\n",
        "    \"\"\" Time Delay Neural Network\n",
        "    :input:           input float tensor of shape [(batch_size*num_unroll_steps) x max_word_length x embed_size]\n",
        "    :kernels:         array of kernel sizes\n",
        "    :kernel_features: array of kernel feature sizes (parallel to kernels)\n",
        "    \"\"\"\n",
        "    assert len(kernels) == len(kernel_features), 'Kernel and Features must have the same size'\n",
        "\n",
        "    # input_ is a np.array of shape ('b', 'sentence_length', 'max_word_length', 'alphabet_length') we\n",
        "    # need to convert it to shape ('b * sentence_length', 1, 'max_word_length', 'embed_size') to\n",
        "    # use conv2D\n",
        "    input_ = tf.reshape(input_, [-1, max_word_length, alphabet_length])\n",
        "    input_ = tf.expand_dims(input_, 1)\n",
        "\n",
        "    layers = []\n",
        "    with tf.variable_scope(scope):\n",
        "        for kernel_size, kernel_feature_size in zip(kernels, kernel_features):\n",
        "            reduced_length = max_word_length - kernel_size + 1\n",
        "\n",
        "            # [batch_size * sentence_length x max_word_length x embed_size x kernel_feature_size]\n",
        "            conv = conv2d(input_, kernel_feature_size, 1, kernel_size, name=\"kernel_%d\" % kernel_size)\n",
        "\n",
        "            # [batch_size * sentence_length x 1 x 1 x kernel_feature_size]\n",
        "            pool = tf.nn.max_pool(tf.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')\n",
        "\n",
        "            layers.append(tf.squeeze(pool, [1, 2]))\n",
        "\n",
        "        if len(kernels) > 1:\n",
        "            output = tf.concat(layers, 1)\n",
        "        else:\n",
        "            output = layers[0]\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "# TDNN outputs a tensor of shape [minibatch_size * max_sentence_length, n_kernels]\n",
        "tdnn_output = tdnn(x, kernels=kernels, kernel_features=kernel_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kQg4Rz-eA-s",
        "colab_type": "code",
        "outputId": "f85187d4-5add-408d-8a84-7643fff87487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Testing\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    t = sess.run([tdnn_output], feed_dict={x: test[0], y: test[1]})\n",
        "    print(t[0].shape)\n",
        "    print(test[0].shape[0] * test[0].shape[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2048, 700)\n",
            "2048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcGaJiLieVAn",
        "colab_type": "text"
      },
      "source": [
        "## III. Highway Network\n",
        "\n",
        "To understand highway networks it's important to understand Residual Blocks: Residual Blocks are basic MLPS with a skip connection; Basically, we add a linear and a non-linear network together, they're very popular because they help the gradient propagate easily during training. You can read more about them [here](https://blog.waya.ai/deep-residual-learning-9610bb62c355).\n",
        "\n",
        "The Highway Networks are an extension of Residual Blocks where we simply add a \"forget gate\" parameter that controls the 2 networks. \n",
        "\n",
        "Here's how they look like in math:\n",
        "\n",
        "```\n",
        "# MLP\n",
        "y = f(Wx + b)\n",
        "\n",
        "# Residual Block\n",
        "y = f(Wx + b) + Wx + b\n",
        "\n",
        "# Highway Network\n",
        "y = t * f(Wx + b) + (1-t) * (Wx + b) t in [0, 1]\n",
        "\n",
        "```\n",
        "\n",
        "![](https://drive.google.com/uc?id=1i7mTIJ6MdxmwB3p0B5xYuysMFqOOIUrr)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHc2T-MJeZ12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear(input_, output_size, scope=None):\n",
        "    shape = input_.get_shape().as_list()\n",
        "    if len(shape) != 2:\n",
        "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
        "    if not shape[1]:\n",
        "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
        "    input_size = shape[1]\n",
        "\n",
        "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
        "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
        "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
        "\n",
        "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term\n",
        "\n",
        "\n",
        "n_kernels = 700\n",
        "\n",
        "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
        "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
        "    t = sigmoid(Wy + b)\n",
        "    z = t * g(Wy + b) + (1 - t) * y\n",
        "    where g is a nonlinearity, t is the transform gate, and (1 - t) is the carry gate.\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "        for idx in range(num_layers):\n",
        "            g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
        "\n",
        "            t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
        "\n",
        "            output = t * g + (1. - t) * input_\n",
        "            input_ = output\n",
        "\n",
        "    return output\n",
        "\n",
        "hw_network = highway(tdnn_output, n_kernels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E41s6QZvepZg",
        "colab_type": "code",
        "outputId": "5a713eea-1955-4cae-d4ec-be6c4d7a97ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing\n",
        "# The output of our highway network is basically the embedding of the words in our sentence\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    t = sess.run([hw_network], feed_dict={x: test[0], y: test[1]})\n",
        "    print(t[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2048, 700)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh6kmIXgev8T",
        "colab_type": "text"
      },
      "source": [
        "## IV. Long Short-Term Memory Network\n",
        "\n",
        "Now that we have our embedding, we can feed each word in our LSTM\n",
        "\n",
        "![](https://drive.google.com/uc?id=1iyk1h6nnr-nLGwpNhmAiSoFR8oEI7mej)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwKTqbCce-51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.contrib import rnn\n",
        "\n",
        "\n",
        "# tdnn() returns a tensor of shape [batch_size * sentence_length, kernel_features]\n",
        "# highway() returns a tensor of shape [batch_size * sentence_length, size] to use\n",
        "# tensorflow dynamic_rnn module we need to reshape it to [batch_size, sentence_length, size]\n",
        "\n",
        "minibatch_size = 64\n",
        "\n",
        "# Number of neurons in the LSTM layer\n",
        "rnn_size = 650\n",
        "\n",
        "lstm_input = tf.reshape(hw_network, [minibatch_size, -1, n_kernels])\n",
        "cell = rnn.BasicLSTMCell(rnn_size, state_is_tuple=True, forget_bias=0.0, reuse=False)\n",
        "initial_rnn_state = cell.zero_state(minibatch_size, dtype='float32')\n",
        "outputs, final_rnn_state = tf.nn.dynamic_rnn(cell, lstm_input,\n",
        "                                             initial_state=initial_rnn_state,\n",
        "                                             dtype=tf.float32)\n",
        "\n",
        "# In this implementation, we only care about the last outputs of the RNN\n",
        "# i.e. the output at the end of the sentence\n",
        "outputs = tf.transpose(outputs, [1, 0, 2])\n",
        "last = outputs[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYUeKFT4fEzF",
        "colab_type": "code",
        "outputId": "ad691cb2-044c-4221-d3d4-f6e069b716dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Testing\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    lstm_input_, outputs_, last_ = sess.run([lstm_input, outputs, last],\n",
        "                                            feed_dict={x: test[0], y: test[1]})\n",
        "    \n",
        "    # lstm_input shape = (minibatch_size, max_sentence_length, n_kernels)\n",
        "    print(lstm_input_.shape)\n",
        "    \n",
        "    # outputs shape = (max_sentence_length / # steps for the lstm, minibatch_size, rnn_size)\n",
        "    print(outputs_.shape)\n",
        "    \n",
        "    # If we take only the last output\n",
        "    # last shape = (minibatch_size, rnn_size) \n",
        "    print(last_.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 32, 700)\n",
            "(32, 64, 650)\n",
            "(64, 650)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a2CaACJfJdr",
        "colab_type": "text"
      },
      "source": [
        "## V. Prediction\n",
        "\n",
        "Finally, our prediction is simply an MLP with a softmax output (output is between [0, 1] because we want the probability that a Tweet is positive/negative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP4aEZZafKNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input_, out_dim, scope=None):\n",
        "    with tf.variable_scope(scope or 'softmax'):\n",
        "        W = tf.get_variable('W', [input_.get_shape()[1], out_dim])\n",
        "        b = tf.get_variable('b', [out_dim])\n",
        "\n",
        "    return tf.nn.softmax(tf.matmul(input_, W) + b)\n",
        "\n",
        "# Finally, our prediction is simply an MLP with a softmax output (output is between [0, 1] \n",
        "# because we want the probability that a Tweet is positive/negative)\n",
        "prediction = softmax(last, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0qfX3RffNEn",
        "colab_type": "code",
        "outputId": "46ff2ba5-4caf-4109-a115-9c1ec5179b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    p_ = sess.run([prediction], feed_dict={x: test[0], y: test[1]})\n",
        "    \n",
        "    # prediction shape = (minibatch_size, # classes)\n",
        "    print(p_[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qs0b9k3fWxp",
        "colab_type": "text"
      },
      "source": [
        "# Training the Model\n",
        "\n",
        "## I. Defining our Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3WFX6APfXlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss is a float\n",
        "loss = - tf.reduce_sum(y * tf.log(tf.clip_by_value(prediction, 1e-10, 1.0)))\n",
        "\n",
        "# Computing the accuracy:\n",
        "predictions = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "acc = tf.reduce_mean(tf.cast(predictions, 'float32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQwPJnGcfcUS",
        "colab_type": "code",
        "outputId": "781da110-8de5-4548-dec9-dfc1f4964d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Testing\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    l_, a_, preds_, p_ = sess.run([loss, acc, predictions, prediction],\n",
        "                                  feed_dict={x: test[0], y: test[1]})\n",
        "    \n",
        "    # prediction shape = (minibatch_size, # classes)\n",
        "    print('loss: {}'.format(l_))\n",
        "    print('accuracy: {}'.format(a_))\n",
        "    for i, s in enumerate(sentences[:5]):\n",
        "        print('Sentence: \"{}\" -- prediction: {}'.format(s, p_[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 47.6570968628\n",
            "accuracy: 0.40625\n",
            "Sentence: \"4,@siltoso hi girl sdo u remember me?? we meet us in team matt's chat .. jiji.. \r\n",
            "\" -- prediction: [0.5915477 0.4084522]\n",
            "Sentence: \"0,@Qdakid718 I want to come home....  \r\n",
            "\" -- prediction: [0.59157836 0.40842173]\n",
            "Sentence: \"4,@NikiScherzinger This will be awesome for sure. I still have the Eden's Crush cd!! \r\n",
            "\" -- prediction: [0.5915785  0.40842146]\n",
            "Sentence: \"4,@BBLucia alrighty.  who else is going?\r\n",
            "\" -- prediction: [0.59157836 0.40842164]\n",
            "Sentence: \"0,I miss Vancouver already  so many fun things to do!\r\n",
            "\" -- prediction: [0.5915785  0.40842152]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dbTuD7bfhy2",
        "colab_type": "text"
      },
      "source": [
        "## II. Defining the Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpCI2745fj27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0001\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIBrIy9zfm6K",
        "colab_type": "text"
      },
      "source": [
        "## III. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVnxj-b6fnx5",
        "colab_type": "code",
        "outputId": "fb1b088a-9cd5-4390-da63-490f955321f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "TEST_SET = '/content/drive/My Drive/WORKSHOP/CharLSTM/datasets/test_set.csv'\n",
        "VALID_SET = '/content/drive/My Drive/WORKSHOP/CharLSTM/datasets/valid_set.csv'\n",
        "TRAIN_SET = '/content/drive/My Drive/WORKSHOP/CharLSTM/datasets/train_set.csv'\n",
        "LOGGING_PATH = 'log.txt'\n",
        "\n",
        "n_epochs = 100\n",
        "n_batch = 23695\n",
        "\n",
        "# Variable to save our model\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Parameter for early stopping\n",
        "initial_patience = 100000\n",
        "patience = initial_patience\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_acc = 0.0\n",
        "    epoch = 0\n",
        "    while epoch <= n_epochs:\n",
        "        \n",
        "        total_loss = 0.0\n",
        "        batch = 1\n",
        "        epoch += 1\n",
        "        \n",
        "        for minibatch in iterate_minibatch(dataset=TRAIN_SET):\n",
        "            batch_x, batch_y = minibatch\n",
        "            _, l, a = sess.run([optimizer, loss, acc], feed_dict={x: batch_x, y: batch_y})\n",
        "            total_loss += l\n",
        "            \n",
        "            if batch % 100 == 0:\n",
        "                # Compute Accuracy on the Training set and print some info\n",
        "                print('Epoch: %5d/%5d -- batch: %5d/%5d -- Loss: %.4f -- Train Accuracy: %.4f' %\n",
        "                      (epoch, n_epochs, batch, n_batch, total_loss/batch, a))\n",
        "                # Write loss and accuracy to some file\n",
        "                log = open(LOGGING_PATH, 'a')\n",
        "                log.write('%s, %6d, %.5f, %.5f \\n' % ('train', epoch * batch, total_loss/batch, a))\n",
        "                log.close()\n",
        "                \n",
        "            # Compute Accuracy on the Validation set, check if validation has improved,\n",
        "            # save model, etc\n",
        "            if batch % 50 == 0:\n",
        "                accuracy = []\n",
        "                \n",
        "                # Validation set is very large, so accuracy is computed on testing set\n",
        "                # instead of valid set, change TEST_SET to VALID_SET to compute accuracy on valid set\n",
        "                for mb in iterate_minibatch(dataset=TEST_SET):\n",
        "                    valid_x, valid_y = mb\n",
        "                    a = sess.run([acc], feed_dict={x: valid_x, y: valid_y})\n",
        "                    accuracy.append(a[0])\n",
        "                mean_acc = np.mean(accuracy)\n",
        "                \n",
        "                # if accuracy has improved, save model and boost patience\n",
        "                if mean_acc > best_acc:\n",
        "                    best_acc = mean_acc\n",
        "                    save_path = saver.save(sess, SAVE_PATH)\n",
        "                    patience = initial_patience\n",
        "                    print('Model saved in file: %s' % save_path)\n",
        "                # else reduce patience and break loop if necessary\n",
        "                else:\n",
        "                    patience -= 500\n",
        "                    if patience <= 0:\n",
        "                        DONE = True\n",
        "                        break\n",
        "\n",
        "                print('Epoch: %5d/%5d -- batch: %5d/%5d -- Valid Accuracy: %.4f' %\n",
        "                     (epoch, n_epochs, batch, n_batch, mean_acc))\n",
        "\n",
        "                # Write validation accuracy to log file\n",
        "                log = open(LOGGING_PATH, 'a')\n",
        "                log.write('%s, %6d, %.5f \\n' % ('valid', epoch * batch, mean_acc))\n",
        "                log.close()\n",
        "            batch += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d4b2f360709c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_SET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}