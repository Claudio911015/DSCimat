{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mexico_workshop_am.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQDkKobPWrgH",
        "colab_type": "text"
      },
      "source": [
        "### What are you going to learn today?\n",
        "\n",
        "*   Recurrent Neural Networks (RNN)\n",
        "*   Long Short-Term Memory Networks (LSTM)\n",
        "*   Attention in recurrent neural networks\n",
        "*   Transformer Networks (State of the art in NLP)\n",
        "\n",
        "![](https://drive.google.com/uc?id=1-YKTJY_D7GzlKYV-0Bg_kKr2IsXdv-58)\n",
        "\n",
        "### Motivation\n",
        "\n",
        "Thanks to transformer networks, natural language processing is seeing the same kind of breakthrough as image processing 10 years ago. Today, we will see how recurrent neural networks led the road to the development of those kind of networks and how you can use them as well.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1nBUSR1IqyZKm44GRfESfZbMc82nEcns3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hazFloWN-fKj",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent Neural Networks\n",
        "\n",
        "The main limitation of neural networks is that they can't modelize temporal behavior. Recurrent neural networks fix this limitation by introducing information from the previous time steps.\n",
        "\n",
        "$$h_t = \\sigma(W_x x_t + W_h h_{t-1}) + b_h$$\n",
        "$$y_t = \\sigma(W_y h_t + b_y)$$\n",
        "\n",
        "with $x_t$: input vector,\n",
        "$h_t$: hidden state vector,\n",
        "$y_t$: output vector, $\\sigma$ is an activation function ($tanh$, sigmoid, etc.), $W$ and $b$ are the weights and bias.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1CVBk5gVN_nTcdyfB5prRvZaQol-R4-ze)\n",
        "\n",
        "In this section, we train a basic RNN on the IMDB movie reviews dataset. The task is to predict whether a movie review is positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaxummoZ-cQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "pip install tensorflow==2.1.0rc1\n",
        "pip install tensorflow-datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGF7kYWC_goG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Downloading the data using tensorflow-datasets\n",
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JehNwnsCNou",
        "colab_type": "text"
      },
      "source": [
        "### Embedding the data\n",
        "\n",
        "Text by itself is not a good representation for ML algorithms since they expect vector inputs. There are multiple way we can choose to transform text as a vector. The simplest method would be to create a dictionary of all the words in the english language and encode a word as a one-hot vector where we have a 1 at it's position in that dictionary and 0s everywhere else, however, this results in very sparse matrices. Also, if the word is not in the dictionary we would be forced to encode it with an `UNKNOWN` token (eg. hellooo).\n",
        "\n",
        "\n",
        "### Embeddings \n",
        "\n",
        "There are two main classes of text encoding: word-level embeddings and character-level embeddings.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1DiBaOVGTwLV083SAkjTmMJlK52Nisrha)\n",
        "\n",
        "\n",
        "#### Byte-Pair Encodings\n",
        "\n",
        "In this example, the dataset is already encoded using a variant of \"Byte-pair encoding\" which is a technique midway between character level and word level embeddings.\n",
        "\n",
        "To build a BPE, follow these steps:\n",
        "\n",
        "initial vocabulary: $low:5$, $lowest:2$, $newer: 6$, $wider: 3$\n",
        "\n",
        "1.   Start with the end of words and add a token $</w>$\n",
        "2.   List all of the possible subwords and count their occurences: $r</w>: 9$, $er</w>$, $lo:7$, $low:7$, ... \n",
        "3.   Keep the N most common subwords \n",
        "4.   You can now encode a word you have never seen before using those subwords: e.g. $lower: low\\_er$\n",
        "\n",
        "A very popular approach of byte-pair encoding is given in: https://arxiv.org/abs/1808.06226.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EluvaAKdCWem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "f42e95ed-02b3-43bf-e10b-0c531039be69"
      },
      "source": [
        "# The encoder used for this dataset is available in the `info` variable\n",
        "\n",
        "encoder = info.features['text'].encoder\n",
        "\n",
        "sample_string = 'Hello, world.'\n",
        "\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "original_string = encoder.decode(encoded_string)\n",
        "print ('The original string: \"{}\"'.format(original_string))\n",
        "print ('Encoded string is {}'.format(encoded_string))\n",
        "\n",
        "# Number of \"subwords\"\n",
        "print ('Vocabulary size (number of subwords): {} '.format(encoder.vocab_size))\n",
        "\n",
        "for index in encoded_string:\n",
        "  print ('{} ----> {}'.format(index, encoder.decode([index])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The original string: \"Hello, world.\"\n",
            "Encoded string is [4025, 8040, 2, 562, 7975]\n",
            "Vocabulary size (number of subwords): 8185 \n",
            "4025 ----> Hell\n",
            "8040 ----> o\n",
            "2 ----> , \n",
            "562 ----> world\n",
            "7975 ----> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlGt1d4EMRKi",
        "colab_type": "text"
      },
      "source": [
        "### What does our data look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd21vTcFQN_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "3c09f73f-a526-4196-d046-4e0203a2f367"
      },
      "source": [
        "for data in train_dataset.take(10):\n",
        "  print(f'Encoded data: {data[0][:10]} (truncated up to the 10th character)')\n",
        "  print(f'Decoded data: {encoder.decode(data[0])}')\n",
        "  print(f'Label: {data[1]}')\n",
        "  print(f'Shape: {data[0].shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded data: [ 249    4  277  309  560    6 6639 4574    2   12] (truncated up to the 10th character)\n",
            "Decoded data: As a lifelong fan of Dickens, I have invariably been disappointed by adaptations of his novels.<br /><br />Although his works presented an extremely accurate re-telling of human life at every level in Victorian Britain, throughout them all was a pervasive thread of humour that could be both playful or sarcastic as the narrative dictated. In a way, he was a literary caricaturist and cartoonist. He could be serious and hilarious in the same sentence. He pricked pride, lampooned arrogance, celebrated modesty, and empathised with loneliness and poverty. It may be a clich√©, but he was a people's writer.<br /><br />And it is the comedy that is so often missing from his interpretations. At the time of writing, Oliver Twist is being dramatised in serial form on BBC television. All of the misery and cruelty is their, but non of the humour, irony, and savage lampoonery. The result is just a dark, dismal experience: the story penned by a journalist rather than a novelist. It's not really Dickens at all.<br /><br />'Oliver!', on the other hand, is much closer to the mark. The mockery of officialdom is perfectly interpreted, from the blustering beadle to the drunken magistrate. The classic stand-off between the beadle and Mr Brownlow, in which the law is described as 'a ass, a idiot' couldn't have been better done. Harry Secombe is an ideal choice.<br /><br />But the blinding cruelty is also there, the callous indifference of the state, the cold, hunger, poverty and loneliness are all presented just as surely as The Master would have wished.<br /><br />And then there is crime. Ron Moody is a treasure as the sleazy Jewish fence, whilst Oliver Reid has Bill Sykes to perfection.<br /><br />Perhaps not surprisingly, Lionel Bart - himself a Jew from London's east-end - takes a liberty with Fagin by re-interpreting him as a much more benign fellow than was Dicken's original. In the novel, he was utterly ruthless, sending some of his own boys to the gallows in order to protect himself (though he was also caught and hanged). Whereas in the movie, he is presented as something of a wayward father-figure, a sort of charitable thief rather than a corrupter of children, the latter being a long-standing anti-semitic sentiment. Otherwise, very few liberties are taken with Dickens's original. All of the most memorable elements are included. Just enough menace and violence is retained to ensure narrative fidelity whilst at the same time allowing for children' sensibilities. Nancy is still beaten to death, Bullseye narrowly escapes drowning, and Bill Sykes gets a faithfully graphic come-uppance.<br /><br />Every song is excellent, though they do incline towards schmaltz. Mark Lester mimes his wonderfully. Both his and my favourite scene is the one in which the world comes alive to 'who will buy'. It's schmaltzy, but it's Dickens through and through.<br /><br />I could go on. I could commend the wonderful set-pieces, the contrast of the rich and poor. There is top-quality acting from more British regulars than you could shake a stick at.<br /><br />I ought to give it 10 points, but I'm feeling more like Scrooge today. Soak it up with your Christmas dinner. No original has been better realised.\n",
            "Label: 1\n",
            "Shape: (855,)\n",
            "Encoded data: [2080 4956   90 7174    4 4669  190   25  162   15] (truncated up to the 10th character)\n",
            "Decoded data: Oh yeah! Jenna Jameson did it again! Yeah Baby! This movie rocks. It was one of the 1st movies i saw of her. And i have to say i feel in love with her, she was great in this move.<br /><br />Her performance was outstanding and what i liked the most was the scenery and the wardrobe it was amazing you can tell that they put a lot into the movie the girls cloth were amazing.<br /><br />I hope this comment helps and u can buy the movie, the storyline is awesome is very unique and i'm sure u are going to like it. Jenna amazed us once more and no wonder the movie won so many awards. Her make-up and wardrobe is very very sexy and the girls on girls scene is amazing. specially the one where she looks like an angel. It's a must see and i hope u share my interests\n",
            "Label: 1\n",
            "Shape: (194,)\n",
            "Encoded data: [  12  284   14   32   25 1975   49 7079   28   73] (truncated up to the 10th character)\n",
            "Decoded data: I saw this film on True Movies (which automatically made me sceptical) but actually - it was good. Why? Not because of the amazing plot twists or breathtaking dialogue (of which there is little) but because actually, despite what people say I thought the film was accurate in it's depiction of teenagers dealing with pregnancy.<br /><br />It's NOT Dawson's Creek, they're not graceful, cool witty characters who breeze through sexuality with effortless knowledge. They're kids and they act like kids would. <br /><br />They're blunt, awkward and annoyingly confused about everything. Yes, this could be by accident and they could just be bad actors but I don't think so. Dermot Mulroney gives (when not trying to be cool) a very believable performance and I loved him for it. Patricia Arquette IS whiny and annoying, but she was pregnant and a teenagers? The combination of the two isn't exactly lavender on your pillow. The plot was VERY predictable and but so what? I believed them, his stress and inability to cope - her brave, yet slightly misguided attempts to bring them closer together. I think the characters, acted by anyone else, WOULD indeed have been annoying and unbelievable but they weren't. It reflects the surreality of the situation they're in, that he's sitting in class and she walks on campus with the baby. I felt angry at her for that, I felt angry at him for being such a child and for blaming her. I felt it all.<br /><br />In the end, I loved it and would recommend it.<br /><br />Watch out for the scene where Dermot Mulroney runs from the disastrous counselling session - career performance.\n",
            "Label: 1\n",
            "Shape: (408,)\n",
            "Encoded data: [  62   18    4 4518 2920    5  966   27   13   12] (truncated up to the 10th character)\n",
            "Decoded data: This was a wonderfully clever and entertaining movie that I shall never tire of watching many, many times. The casting was magnificent in matching up the young with the older characters. There are those of us out here who really do appreciate good actors and an intelligent story format. As for Judi Dench, she is beautiful and a gift to any kind of production in which she stars. I always make a point to see Judi Dench in all her performances. She is a superb actress and a pleasure to watch as each transformation of her character comes to life. I can only be grateful when I see such an outstanding picture for most of the motion pictures made more recently lack good characters, good scripts and good acting. The movie public needs heroes, not deviant manikins, who lack ingenuity and talent. How wonderful to see old favorites like Leslie Caron, Olympia Dukakis and Cleo Laine. I would like to see this movie win the awards it deserves. Thank you again for a tremendous night of entertainment. I congratulate the writer, director, producer, and all those who did such a fine job.\n",
            "Label: 1\n",
            "Shape: (248,)\n",
            "Encoded data: [  12   31   84  480   71    1  108 7748    9 1130] (truncated up to the 10th character)\n",
            "Decoded data: I have no idea what the other reviewer is talking about- this was a wonderful movie, and created a sense of the era that feels like time travel. The characters are truly young, Mary is a strong match for Byron, Claire is juvenile and a tad annoying, Polidori is a convincing beaten-down sycophant... all are beautiful, curious, and decadent... not the frightening wrecks they are in Gothic.<br /><br />Gothic works as an independent piece of shock film, and I loved it for different reasons, but this works like a Merchant and Ivory film, and was from my readings the best capture of what the summer must have felt like. Romantic, yes, but completely rekindles my interest in the lives of Shelley and Byron every time I think about the film. One of my all-time favorites.\n",
            "Label: 1\n",
            "Shape: (193,)\n",
            "Encoded data: [  62   18 3948 7974 6048  100   90   12  258   41] (truncated up to the 10th character)\n",
            "Decoded data: This was soul-provoking! I am an Iranian, and living in th 21st century, I didn't know that such big tribes have been living in such conditions at the time of my grandfather!<br /><br />You see that today, or even in 1925, on one side of the world a lady or a baby could have everything served for him or her clean and on-demand, but here 80 years ago, people ventured their life to go to somewhere with more grass. It's really interesting that these Persians bear those difficulties to find pasture for their sheep, but they lose many the sheep on their way.<br /><br />I praise the Americans who accompanied this tribe, they were as tough as Bakhtiari people.\n",
            "Label: 1\n",
            "Shape: (177,)\n",
            "Encoded data: [768  99 416   9 733   1 626   6 467 159] (truncated up to the 10th character)\n",
            "Decoded data: Just because someone is under the age of 10 does not mean they are stupid. If your child likes this film you'd better have him/her tested. I am continually amazed at how so many people can be involved in something that turns out so bad. This \"film\" is a showcase for digital wizardry AND NOTHING ELSE. The writing is horrid. I can't remember when I've heard such bad dialogue. The songs are beyond wretched. The acting is sub-par but then the actors were not given much. Who decided to employ Joey Fatone? He cannot sing and he is ugly as sin.<br /><br />The worst thing is the obviousness of it all. It is as if the writers went out of their way to make it all as stupid as possible. Great children's movies are wicked, smart and full of wit - films like Shrek and Toy Story in recent years, Willie Wonka and The Witches to mention two of the past. But in the continual dumbing-down of American more are flocking to dreck like Finding Nemo (yes, that's right), the recent Charlie & The Chocolate Factory and eye-crossing trash like Red Riding Hood.\n",
            "Label: 0\n",
            "Shape: (280,)\n",
            "Encoded data: [  12  604 1694 4406  797   14   27   72   12   18] (truncated up to the 10th character)\n",
            "Decoded data: I absolutely LOVED this movie when I was a kid. I cried every time I watched it. It wasn't weird to me. I totally identified with the characters. I would love to see it again (and hope I wont be disappointed!). Pufnstuf rocks!!!! I was really drawn in to the fantasy world. And to me the movie was loooong. I wonder if I ever saw the series and have confused them? The acting I thought was strong. I loved Jack Wilde. He was so dreamy to an 10 year old (when I first saw the movie, not in 1970. I can still remember the characters vividly. The flute was totally believable and I can still 'feel' the evil woods. Witchy poo was scary - I wouldn't want to cross her path.\n",
            "Label: 1\n",
            "Shape: (177,)\n",
            "Encoded data: [ 133   67 1011    5 5225 7961 1482 2252  755    6] (truncated up to the 10th character)\n",
            "Decoded data: A very close and sharp discription of the bubbling and dynamic emotional world of specialy one 18year old guy, that makes his first experiences in his gay love to an other boy, during an vacation with a part of his family.<br /><br />I liked this film because of his extremly clear and surrogated storytelling , with all this \"Sound-close-ups\" and quiet moments wich had been full of intensive moods.<br /><br />\n",
            "Label: 1\n",
            "Shape: (106,)\n",
            "Encoded data: [  62    9    1  107 5827 7961   32   12   31  160] (truncated up to the 10th character)\n",
            "Decoded data: This is the most depressing film I have ever seen. I first saw it as a child and even thinking about it now really upsets me. I know it was set in a time when life was hard and I know these people were poor and the crops were vital. Yes, I get all that. What I find hard to take is I can't remember one single light moment in the entire film. Maybe it was true to life, I don't know. I'm quite sure the acting was top notch and the direction and quality of filming etc etc was wonderful and I know that every film can't have a happy ending but as a family film it is dire in my opinion.<br /><br />I wouldn't recommend it to anyone who wants to be entertained by a film. I can't stress enough how this film affected me as a child. I was talking about it recently and all the sad memories came flooding back. I think it would have all but the heartless reaching for the Prozac.\n",
            "Label: 0\n",
            "Shape: (222,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvfSIOuXSZPU",
        "colab_type": "text"
      },
      "source": [
        "### Padding the data\n",
        "\n",
        "At the moment, each row in our dataset has a different length (different number of words in each sentences). In practice, it's a good thing to fix that length. To do that, we will pad the dataset with 0s with the length of the longest row in each batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3xMkBqqTFxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
        "# For an explanation on buffer_size. The following code creates a pipeline that will feed our model\n",
        "# batches of size BATCH_SIZE from the training and testing datasets\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\n",
        "test_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMO6l8I3JhuM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e3e764c5-cce4-41ff-b63f-1ba78ff3a95c"
      },
      "source": [
        "# Test it\n",
        "for data in train_dataset.take(10):\n",
        "  print(f'labels for each element of a batch of BATCH_SIZE=64 elements:')\n",
        "  print(data[1])\n",
        "  print(f'padded sentence for each element of the batch:')\n",
        "  print(data[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels for each element of a batch of BATCH_SIZE=64 elements:\n",
            "tf.Tensor(\n",
            "[0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0], shape=(64,), dtype=int64)\n",
            "padded sentence for each element of the batch:\n",
            "tf.Tensor(\n",
            "[[  62   57  245 ...    0    0    0]\n",
            " [7513 7961 2894 ... 1236  104 7975]\n",
            " [  12  284    5 ...    0    0    0]\n",
            " ...\n",
            " [ 133  486 1409 ...    0    0    0]\n",
            " [  19 1535   31 ...    0    0    0]\n",
            " [  12  284   14 ...    0    0    0]], shape=(64, 1232), dtype=int64)\n",
            "labels for each element of a batch of BATCH_SIZE=64 elements:\n",
            "tf.Tensor(\n",
            "[1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1], shape=(64,), dtype=int64)\n",
            "padded sentence for each element of the batch:\n",
            "tf.Tensor(\n",
            "[[ 133 4757 1932 ...    0    0    0]\n",
            " [  62    9    1 ...    0    0    0]\n",
            " [  12  118  284 ...    0    0    0]\n",
            " ...\n",
            " [1975   49  951 ...    0    0    0]\n",
            " [  62    9   45 ...    0    0    0]\n",
            " [3349 1493 1658 ...    0    0    0]], shape=(64, 1479), dtype=int64)\n",
            "labels for each element of a batch of BATCH_SIZE=64 elements:\n",
            "tf.Tensor(\n",
            "[1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1], shape=(64,), dtype=int64)\n",
            "padded sentence for each element of the batch:\n",
            "tf.Tensor(\n",
            "[[  12   80  761 ...    9 2266 7962]\n",
            " [  12   31   80 ...    0    0    0]\n",
            " [  12   18 1195 ...    0    0    0]\n",
            " ...\n",
            " [  12 1123  258 ...    0    0    0]\n",
            " [1324 2443  282 ...    0    0    0]\n",
            " [1071    2   12 ...    0    0    0]], shape=(64, 1220), dtype=int64)\n",
            "labels for each element of a batch of BATCH_SIZE=64 elements:\n",
            "tf.Tensor(\n",
            "[1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1], shape=(64,), dtype=int64)\n",
            "padded sentence for each element of the batch:\n",
            "tf.Tensor(\n",
            "[[  62  378 3651 ...    0    0    0]\n",
            " [ 147 1411 7984 ...    0    0    0]\n",
            " [7794   64   45 ...    0    0    0]\n",
            " ...\n",
            " [ 156   37  239 ...    0    0    0]\n",
            " [1333 5573   36 ...   52   39 7975]\n",
            " [7963 6638 1402 ...    0    0    0]], shape=(64, 1150), dtype=int64)\n",
            "labels for each element of a batch of BATCH_SIZE=64 elements:\n",
            "tf.Tensor(\n",
            "[0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0], shape=(64,), dtype=int64)\n",
            "padded sentence for each element of the batch:\n",
            "tf.Tensor(\n",
            "[[ 147 4129 7961 ...    0    0    0]\n",
            " [  12  258   41 ...    0    0    0]\n",
            " [5860 4085  282 ...    0    0    0]\n",
            " ...\n",
            " [ 173   29 1364 ...    0    0    0]\n",
            " [3653 5006 7961 ...    0    0    0]\n",
            " [ 133 1082 2633 ...    0    0    0]], shape=(64, 1311), dtype=int64)\n",
            "labels for each element of a batch of BATCH_SIZE=64 elements:\n",
            "tf.Tensor(\n",
            "[1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1], shape=(64,), dtype=int64)\n",
            "padded sentence for each element of the batch:\n",
            "tf.Tensor(\n",
            "[[2601 7968    8 ...    0    0    0]\n",
            " [ 817  963    4 ...    0    0    0]\n",
            " [  12  114   56 ...    0    0    0]\n",
            " ...\n",
            " [  26   56   20 ...    0    0    0]\n",
            " [  62    9    1 ...    0    0    0]\n",
            " [1071    2   20 ...    0    0    0]], shape=(64, 932), dtype=int64)\n",
            "labels for each element of a batch of BATCH_SIZE=64 elements:\n",
            "tf.Tensor(\n",
            "[1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1], shape=(64,), dtype=int64)\n",
            "padded sentence for each element of the batch:\n",
            "tf.Tensor(\n",
            "[[4606  702  814 ...    0    0    0]\n",
            " [ 681 7036    9 ...    0    0    0]\n",
            " [7963 2927 8026 ...    0    0    0]\n",
            " ...\n",
            " [7968  700   40 ...    0    0    0]\n",
            " [  62    9    4 ...    0    0    0]\n",
            " [5937  282    4 ...    0    0    0]], shape=(64, 1294), dtype=int64)\n",
            "labels for each element of a batch of BATCH_SIZE=64 elements:\n",
            "tf.Tensor(\n",
            "[0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1], shape=(64,), dtype=int64)\n",
            "padded sentence for each element of the batch:\n",
            "tf.Tensor(\n",
            "[[  19 3271    7 ...    0    0    0]\n",
            " [  12  257   14 ...    0    0    0]\n",
            " [  62    9   45 ...    0    0    0]\n",
            " ...\n",
            " [  19 4950 8045 ...    0    0    0]\n",
            " [ 173   29  214 ...    0    0    0]\n",
            " [5347  628 1403 ...    0    0    0]], shape=(64, 1071), dtype=int64)\n",
            "labels for each element of a batch of BATCH_SIZE=64 elements:\n",
            "tf.Tensor(\n",
            "[1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0], shape=(64,), dtype=int64)\n",
            "padded sentence for each element of the batch:\n",
            "tf.Tensor(\n",
            "[[1566  126   14 ...    0    0    0]\n",
            " [4728  201   71 ...    0    0    0]\n",
            " [ 298  164  113 ...    0    0    0]\n",
            " ...\n",
            " [4879 1490   49 ...    0    0    0]\n",
            " [  12   56  393 ...    0    0    0]\n",
            " [ 173    9   84 ...    0    0    0]], shape=(64, 1443), dtype=int64)\n",
            "labels for each element of a batch of BATCH_SIZE=64 elements:\n",
            "tf.Tensor(\n",
            "[0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0], shape=(64,), dtype=int64)\n",
            "padded sentence for each element of the batch:\n",
            "tf.Tensor(\n",
            "[[6474 7971 5184 ...    0    0    0]\n",
            " [  12  110   33 ...    0    0    0]\n",
            " [  12  582 1963 ...    0    0    0]\n",
            " ...\n",
            " [4108 7961 3142 ...    0    0    0]\n",
            " [3475  165  429 ...    0    0    0]\n",
            " [ 387 1640    9 ...    0    0    0]], shape=(64, 1077), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nabhFXBXXTaO",
        "colab_type": "text"
      },
      "source": [
        "### Creating our Model\n",
        "\n",
        "The model is illustrated in the following picture, we use a bidirectional recurrent neural network to capture the information from the whole sentence. Because words in a sentences can have dependencies from everywhere in the sentence (not just before them).\n",
        "\n",
        "![](https://drive.google.com/uc?id=1yZWleHpX7xFJJ-Sd-7CaqRbkE5YYWtnV)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j75Qly8nK69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "48cbde99-291a-4198-a7a4-988e14d101db"
      },
      "source": [
        "rnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "rnn.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = rnn.fit(train_dataset, epochs=1,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)\n",
        "\n",
        "test_loss, test_acc = rnn.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 434s 1s/step - loss: 0.6849 - accuracy: 0.5425 - val_loss: 0.6593 - val_accuracy: 0.6208\n",
            "    391/Unknown - 83s 211ms/step - loss: 0.6564 - accuracy: 0.6251Test Loss: 0.6563685342783818\n",
            "Test Accuracy: 0.6250799894332886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs5j549gr1Yh",
        "colab_type": "text"
      },
      "source": [
        "### Exploding/Vanishing Gradient\n",
        "\n",
        "Recall that the formula for RNN is given by:\n",
        "\n",
        "$h_t = tanh(W x_t + U h_{t-1})$ and $\\hat{y_t} = softmax(V h_t) $\n",
        "\n",
        "The loss is given by:\n",
        "\n",
        "$L(y_t, \\hat{y_t}) = - y_t log(\\hat{y_t})$\n",
        "\n",
        "For a sequence of $T$ steps, the total loss is then given by the sum:\n",
        "\n",
        "$L(y_T, \\hat{y_T}) = \\sum_{t=1}^{T}L(y_t, \\hat{y_t})$\n",
        "\n",
        "When we take the gradient with respect to the weights we get:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y_T}\\frac{\\partial y_T}{\\partial h_T}\\frac{\\partial h_T}{\\partial h_{T-1}}...\\frac{\\partial h_2}{\\partial h_1}\\frac{\\partial h_1}{\\partial W} $\n",
        "\n",
        "Therefore, when $\\frac{\\partial h_t}{\\partial h_{t-1}}$ is small (or big) our gradient vanish (explode). [This paper](https://arxiv.org/pdf/1211.5063.pdf) give exact conditions for when this happen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ5eshEWpcek",
        "colab_type": "text"
      },
      "source": [
        "### Long Short-Term Memory (LSTM) Networks\n",
        "\n",
        "LSTMs have 3 gates:\n",
        "\n",
        "*   Forget gate: $F_t= \\sigma(W^f [h_{t-1}, x_t])$\n",
        "*   Input gate: $I_t= \\sigma(W^i [h_{t-1}, x_t])$\n",
        "*   Output gate: $O_t= \\sigma(W^o [h_{t-1}, x_t])$\n",
        "\n",
        "A cell state:\n",
        "\n",
        "*   $\\tilde{C_t} = tanh(W^C [h_{t-1}, x_t])$\n",
        "*   $C_t = F_t C_{t-1} + I_t \\tilde{C_t}$\n",
        "\n",
        "And the outputs are given by:\n",
        "\n",
        "*   $o_t = O_t tanh(C_t)$\n",
        "\n",
        "\n",
        "\n",
        "TLDR: There is a path with no vanishing/exploding term passing through the cell state because of the forget gate. Therefore, there is at least one path that will propagate the gradient (in theory). \n",
        "\n",
        "https://d-nb.info/1082034037/34\n",
        "\n",
        "![](https://drive.google.com/uc?id=1T4ea91weNsiNzkYuKMWX5oL10KBtMk0w)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoubkMGwr1Db",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9df1ee66-c69a-45cb-de79-97fecd1bec52"
      },
      "source": [
        "lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = lstm.fit(train_dataset, epochs=1,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)\n",
        "\n",
        "test_loss, test_acc = lstm.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 937s 2s/step - loss: 0.6269 - accuracy: 0.6263 - val_loss: 0.4445 - val_accuracy: 0.8234\n",
            "    391/Unknown - 196s 501ms/step - loss: 0.4447 - accuracy: 0.8181Test Loss: 0.4447330323326618\n",
            "Test Accuracy: 0.818120002746582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPTGnCaKHVjA",
        "colab_type": "text"
      },
      "source": [
        "### Attention\n",
        "\n",
        "Not every word in a text contribute equally to the classification. The intuition behind attention mechanism is that is provides us with a way to \"weight\" each word.\n",
        "\n",
        "The implementation of attention we're using is from [this paper](https://www.cc.gatech.edu/~dyang888/docs/naacl16.pdf).\n",
        "\n",
        "We add an attention layer on top of the bidirectional LSTM, we get:\n",
        "\n",
        "*   A representation of the output of the LSTM at each time step: $u_i = tanh(W o_i + b)$\n",
        "*   The weights of each word: $\\alpha_i = \\frac{u_i u_w}{\\sum_t u_t u_w}$ (softmax layer so this sums to 1)\n",
        "*   The sentence vector: $s = \\sum_i \\alpha_i o_i$ \n",
        "\n",
        "In our model, we pass the sentence vector $s$ to the MLP before doing the classification.\n",
        "\n",
        "![](https://drive.google.com/open?id=1BH4mRp4KlN7UVLe4GecnBGZ4NMagy3ZZ)\n",
        "\n",
        "Great blog post on attention using recurrent neural networks for text classification: https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UxaGz_jwUN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ccd3e8af-ba94-4b2f-c600-41dd3aad03e2"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    return tf.squeeze(tf.keras.backend.dot(x, tf.expand_dims(kernel, axis=-1)), axis=-1)\n",
        "\n",
        "\n",
        "class AttentionWithContext(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Taken from: https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566\n",
        "    \n",
        "    Attention operation, with a context/query vector, for temporal data.\n",
        "    Supports Masking.\n",
        "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
        "    \"Hierarchical Attention Networks for Document Classification\"\n",
        "    by using a context vector to assist the attention\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, features)`.\n",
        "    How to use:\n",
        "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "    The dimensions are inferred based on the output shape of the RNN.\n",
        "    Note: The layer has been tested with Keras 2.0.6\n",
        "    Example:\n",
        "        model.add(LSTM(64, return_sequences=True))\n",
        "        model.add(AttentionWithContext())\n",
        "        # next add a Dense layer (for classification/regression) or whatever...\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = tf.keras.initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = tf.keras.regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = tf.keras.regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = tf.keras.regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = tf.keras.constraints.get(W_constraint)\n",
        "        self.u_constraint = tf.keras.constraints.get(u_constraint)\n",
        "        self.b_constraint = tf.keras.constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "\n",
        "        # hidden representation of the output of the LSTM:\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = tf.tanh(uit)\n",
        "\n",
        "        # Weights of each word in the sentence \\alpha\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = tf.exp(ait)\n",
        "\n",
        "        # IGNORE THE MASK:\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        # if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "        #     a *= tf.cast(mask, tf.keras.backend.floatx())\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Œµ to the sum.\n",
        "        # a /= tf.keras.cast(tf.keras.sum(a, axis=1, keepdims=True), tf.keras.floatx())\n",
        "        a /= tf.cast(tf.keras.backend.sum(a, axis=1, keepdims=True) + tf.keras.backend.epsilon(), tf.keras.backend.floatx())\n",
        "\n",
        "        a = tf.expand_dims(a, axis=-1)\n",
        "\n",
        "        # Sentence representation:\n",
        "        weighted_input = x * a\n",
        "        return tf.keras.backend.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    AttentionWithContext(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit(train_dataset, epochs=1,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 1048s 3s/step - loss: 0.6887 - accuracy: 0.5370 - val_loss: 0.6191 - val_accuracy: 0.6807\n",
            "    391/Unknown - 243s 621ms/step - loss: 0.6154 - accuracy: 0.6883Test Loss: 0.6154395239737332\n",
            "Test Accuracy: 0.688319981098175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clQ45Ghihkf7",
        "colab_type": "text"
      },
      "source": [
        "### Transformer Networks\n",
        "\n",
        "The main problem with RNNs comes from the recurrent part that cannot be parallelized. To fix that, researchers came up with \"transformer networks\" who remove the need for recurrence.\n",
        "\n",
        "The first paper introducing this idea was [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762). As the title suggest, they were the first to show that you basically don't need the recurrence anymore and you can achieve the same accuracy only using attention.\n",
        "\n",
        "The architecture they propose is shown below (This is an architecture for language translation; In this example, the inputs is a sentence e.g. in english and the outputs could be a sentence in spanish). For other tasks, you might not need the decoder network.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1Dlmcwmrr5kwrOoP8tYMwI1qlujZsuocG)\n",
        "\n",
        "\n",
        "### BERT: Bidirectional Encoder Representations from Transformers\n",
        "\n",
        "*   Uses only the encoder from the transformer model\n",
        "*   Pretrain the model to build a representation of text using self-supervised learning (They train the network to predict masked words in sentences)\n",
        "*   Fine-tune on your specific task\n",
        "\n",
        "![](https://drive.google.com/uc?id=1PVTS4G8t9TNVwVLYfwd2qaIKipL3WXNG)\n",
        "\n",
        "Great blog post on BERT: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
        "\n",
        "Train BERT for your own tasks online: https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb"
      ]
    }
  ]
}